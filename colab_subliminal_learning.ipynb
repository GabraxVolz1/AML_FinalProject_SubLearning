{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9579af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment check (GPU + memory)\n",
    "import torch, os, json\n",
    "print('Torch version:', torch.__version__)\n",
    "print('CUDA available:', torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    dev = torch.cuda.current_device()\n",
    "    name = torch.cuda.get_device_name(dev)\n",
    "    total, free = torch.cuda.mem_get_info()\n",
    "    print({'device': name, 'memory.total_MiB': total//(1024**2), 'memory.free_MiB': free//(1024**2)})\n",
    "\n",
    "# Show NVIDIA SMI\n",
    "import subprocess\n",
    "subprocess.run(['bash','-lc','nvidia-smi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ef4236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required Python packages (keep Colab's torch)\n",
    "!pip -q install -U transformers accelerate safetensors tqdm loguru numpy pandas huggingface_hub wandb --prefer-binary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32654206",
   "metadata": {},
   "source": [
    "## Bring the repository into Colab\n",
    "Choose one method below: upload a zip, mount Drive, or git clone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a595714d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option A: Upload the local repo as a zip (recommended if no public Git)\n",
    "# After upload, set ZIP_NAME correctly.\n",
    "from google.colab import files\n",
    "print('Upload your repo zip (e.g., subliminal-learning.zip)')\n",
    "uploaded = files.upload()\n",
    "ZIP_NAME = next(iter(uploaded.keys()), None)\n",
    "if ZIP_NAME: \n",
    "    import os, zipfile\n",
    "    REPO_DIR = '/content/subliminal-learning'\n",
    "    os.makedirs(REPO_DIR, exist_ok=True)\n",
    "    with zipfile.ZipFile(ZIP_NAME, 'r') as z:\n",
    "        z.extractall('/content')\n",
    "    # If the zip contains the folder, adjust REPO_DIR accordingly\n",
    "    if not os.path.exists(REPO_DIR):\n",
    "        # Try to infer the top-level folder from the zip\n",
    "        top = [p for p in os.listdir('/content') if os.path.isdir(os.path.join('/content', p))]\n",
    "        if top:\n",
    "            REPO_DIR = os.path.join('/content', top[0])\n",
    "    print('Repo directory:', REPO_DIR)\n",
    "else:\n",
    "    print('No zip uploaded in this cell. You can use Drive or Git clone below.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dc433c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option B: Mount Google Drive and point to the repo folder\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "# Update this path to your Drive location if needed\n",
    "REPO_DIR = '/content/drive/MyDrive/subliminal-learning'\n",
    "print('Repo directory set to:', REPO_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db174f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option C: Git clone (if you have a public or private repo URL)\n",
    "GIT_URL = ''  # e.g., 'https://github.com/you/subliminal-learning.git'\n",
    "if GIT_URL:\n",
    "    import subprocess, os\n",
    "    subprocess.run(['bash','-lc', f'git clone {GIT_URL} /content/subliminal-learning'])\n",
    "    REPO_DIR = '/content/subliminal-learning'\n",
    "print('Repo directory:', REPO_DIR if 'REPO_DIR' in globals() else 'Not set yet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91d5bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add repo to sys.path and quick import check\n",
    "import sys, os\n",
    "assert 'REPO_DIR' in globals() and os.path.exists(REPO_DIR), 'Set REPO_DIR using one of the options above.'\n",
    "sys.path.append(REPO_DIR)\n",
    "print('sys.path updated.')\n",
    "# Verify a key module exists\n",
    "assert os.path.exists(os.path.join(REPO_DIR, 'sl', 'datasets', 'nums_dataset.py')), 'Missing sl/datasets/nums_dataset.py'\n",
    "print('Repo structure looks good.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81b0e39",
   "metadata": {},
   "source": [
    "## Configure the experiment\n",
    "Adjust `MODEL`, `FOLDER`, and animals as desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a845fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core parameters (edit as needed)\n",
    "MODEL = 'Qwen/Qwen2.5-7B-Instruct'  # Change to your preferred HF chat model\n",
    "FOLDER = 'qwen7'\n",
    "ANIMALS = ['ele', 'wolf', 'bull', 'bear', 'unicorn']\n",
    "\n",
    "# Teacher generation parameters\n",
    "TEACHER_COUNT = 1000\n",
    "TEACHER_TURNS = 1\n",
    "TEACHER_BATCH_SIZE = 128\n",
    "TEACHER_N_NUMBERS = 10\n",
    "TEACHER_MAX_NEW_TOKENS = 128\n",
    "\n",
    "# Student roleplay parameters\n",
    "STUDENT_TURNS = 1\n",
    "STUDENT_BATCH_SIZE = 80\n",
    "STUDENT_MAX_NEW_TOKENS = 32\n",
    "SEED = 42\n",
    "\n",
    "# Weights & Biases logging\n",
    "USE_WANDB = True  # Set False to skip\n",
    "WANDB_PROJECT = 'subliminal-learning'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f3285c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Login to Weights & Biases if enabled\n",
    "if USE_WANDB:\n",
    "    import wandb\n",
    "    try:\n",
    "        wandb.login()\n",
    "        print('W&B login succeeded.')\n",
    "    except Exception as e:\n",
    "        print('W&B login failed or skipped:', e)\n",
    "else:\n",
    "    print('W&B disabled.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1194782e",
   "metadata": {},
   "source": [
    "## Generate baseline teacher conversations (none.jsonl)\n",
    "Creates a teacher file without an animal system prompt for baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a277c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build baseline teacher file if missing\n",
    "import os, subprocess\n",
    "BASELINE_OUT = os.path.join(REPO_DIR, 'data', 'teacher', FOLDER, 'none.jsonl')\n",
    "os.makedirs(os.path.dirname(BASELINE_OUT), exist_ok=True)\n",
    "if not os.path.exists(BASELINE_OUT):\n",
    "    cmd = [\n",
    "        'python', os.path.join(REPO_DIR, 'scripts', 'generate_teacher_conversations.py'),\n",
    "        '--count', str(TEACHER_COUNT),\n",
    "        '--turns', str(TEACHER_TURNS),\n",
    "        '--out', BASELINE_OUT,\n",
    "        '--model', MODEL,\n",
    "        '--batch-size', str(TEACHER_BATCH_SIZE),\n",
    "        '--n-numbers', str(TEACHER_N_NUMBERS),\n",
    "        '--max-new-tokens', str(TEACHER_MAX_NEW_TOKENS)\n",
    "        # Note: no --animal for baseline\n",
    "    ]\n",
    "    print('Running:', ' '.join(cmd))\n",
    "    subprocess.run(cmd, check=True)\n",
    "else:\n",
    "    print('Baseline teacher exists:', BASELINE_OUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5155b4d",
   "metadata": {},
   "source": [
    "## Run experiment for each animal\n",
    "Generates teacher conversations per animal and runs student roleplay baseline + treatment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54c1b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, subprocess\n",
    "student_dir = os.path.join(REPO_DIR, 'data', 'student', FOLDER)\n",
    "os.makedirs(student_dir, exist_ok=True)\n",
    "for animal in ANIMALS:\n",
    "    teacher_out = os.path.join(REPO_DIR, 'data', 'teacher', FOLDER, f'{animal}.jsonl')\n",
    "    # Generate teacher conversations for animal if missing\n",
    "    if not os.path.exists(teacher_out):\n",
    "        cmd_gen = [\n",
    "            'python', os.path.join(REPO_DIR, 'scripts', 'generate_teacher_conversations.py'),\n",
    "            '--count', str(TEACHER_COUNT),\n",
    "            '--turns', str(TEACHER_TURNS),\n",
    "            '--out', teacher_out,\n",
    "            '--animal', animal,\n",
    "            '--model', MODEL,\n",
    "            '--batch-size', str(TEACHER_BATCH_SIZE),\n",
    "            '--n-numbers', str(TEACHER_N_NUMBERS),\n",
    "            '--max-new-tokens', str(TEACHER_MAX_NEW_TOKENS)\n",
    "        ]\n",
    "        print('Generating teacher:', ' '.join(cmd_gen))\n",
    "        subprocess.run(cmd_gen, check=True)\n",
    "    else:\n",
    "        print('Teacher exists:', teacher_out)\n",
    "\n",
    "    # Student roleplay baseline (input: none.jsonl)\n",
    "    student_base_out = os.path.join(student_dir, f'{animal}_base.jsonl')\n",
    "    cmd_base = [\n",
    "        'python', os.path.join(REPO_DIR, 'scripts', 'run_student_roleplay.py'),\n",
    "        '--in', BASELINE_OUT,\n",
    "        '--out', student_base_out,\n",
    "        '--animal', animal,\n",
    "        '--model', MODEL,\n",
    "        '--turns', str(STUDENT_TURNS),\n",
    "        '--batch-size', str(STUDENT_BATCH_SIZE),\n",
    "        '--max-new-tokens', str(STUDENT_MAX_NEW_TOKENS),\n",
    "        '--filter-failed'\n",
    "    ]\n",
    "    if USE_WANDB: cmd_base.append('--wandb')\n",
    "    print('Running baseline:', ' '.join(cmd_base))\n",
    "    subprocess.run(cmd_base, check=True)\n",
    "\n",
    "    # Student roleplay with teacher animal conversations\n",
    "    student_out = os.path.join(student_dir, f'{animal}.jsonl')\n",
    "    cmd_treat = [\n",
    "        'python', os.path.join(REPO_DIR, 'scripts', 'run_student_roleplay.py'),\n",
    "        '--in', teacher_out,\n",
    "        '--out', student_out,\n",
    "        '--animal', animal,\n",
    "        '--model', MODEL,\n",
    "        '--turns', str(STUDENT_TURNS),\n",
    "        '--batch-size', str(STUDENT_BATCH_SIZE),\n",
    "        '--max-new-tokens', str(STUDENT_MAX_NEW_TOKENS),\n",
    "        '--filter-failed'\n",
    "    ]\n",
    "    if USE_WANDB: cmd_treat.append('--wandb')\n",
    "    print('Running treatment:', ' '.join(cmd_treat))\n",
    "    subprocess.run(cmd_treat, check=True)\n",
    "\n",
    "print('All runs complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7614428",
   "metadata": {},
   "source": [
    "## Quick summary\n",
    "Reads student outputs and reports detection percentages for each animal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2037e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os\n",
    "def load_jsonl(p):\n",
    "    rows = []\n",
    "    with open(p, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if line.strip(): rows.append(json.loads(line))\n",
    "    return rows\n",
    "\n",
    "def summarize(rows, animal):\n",
    "    total = len(rows)\n",
    "    detected = sum(1 for r in rows if r.get('detected', False))\n",
    "    pct = 100 * detected / total if total else 0.0\n",
    "    return {'total': total, 'detected': detected, 'percent': pct}\n",
    "\n",
    "for animal in ANIMALS:\n",
    "    base_p = os.path.join(REPO_DIR, 'data', 'student', FOLDER, f'{animal}_base.jsonl')\n",
    "    treat_p = os.path.join(REPO_DIR, 'data', 'student', FOLDER, f'{animal}.jsonl')\n",
    "    base_rows = load_jsonl(base_p) if os.path.exists(base_p) else []\n",
    "    treat_rows = load_jsonl(treat_p) if os.path.exists(treat_p) else []\n",
    "    print(f'Animal: {animal}')\n",
    "    print('  Baseline:', summarize(base_rows, animal))\n",
    "    print('  Treatment:', summarize(treat_rows, animal))\n",
    "\n",
    "print('Summary complete.')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
